{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiNn8HK0A_Zn",
        "outputId": "3c5c193e-6f69-415c-d53e-3282357d56e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Qwen'...\n",
            "remote: Enumerating objects: 1753, done.\u001b[K\n",
            "remote: Counting objects: 100% (842/842), done.\u001b[K\n",
            "remote: Compressing objects: 100% (321/321), done.\u001b[K\n",
            "remote: Total 1753 (delta 635), reused 605 (delta 506), pack-reused 911 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1753/1753), 35.93 MiB | 12.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1034/1034), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/QwenLM/Qwen.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Qwen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfcjEh1JBNGc",
        "outputId": "efb3d6d5-5215-44d7-8206-091050c0d0b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Qwen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed transformers==4.32.0 peft pydantic==1.10.13 transformers_stream_generator einops tiktoken optimum auto-gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQqKjAo3BPdu",
        "outputId": "3d4d904b-9b4a-42e6-d0e9-3e5ae7cc8585"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.15.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.32.0\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydantic==1.10.13\n",
            "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers_stream_generator\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting optimum\n",
            "  Downloading optimum-1.22.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.13) (4.12.2)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "INFO: pip is looking at multiple versions of deepspeed to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.15.0.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading deepspeed-0.14.5.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.4.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Collecting coloredlogs (from optimum)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.13.3)\n",
            "Collecting datasets (from optimum)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.45.0,>=4.29->optimum) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets->optimum)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (2.1.4)\n",
            "Collecting xxhash (from datasets->optimum)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets->optimum)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.10.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2024.1)\n",
            "Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.22.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.7/453.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed, transformers_stream_generator\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.14.5-py3-none-any.whl size=1468739 sha256=5cdadc2cb6d1cf79ff48abac28883146f6e01c15ba6afd5d944b546bcc663e8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/4f/43/e69eacbe24afb789cb4a7691938cb375cea39a042a9a28955e\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12425 sha256=8157030b8b2e23311538ffb492abcc6b045ae3dc6d1e14e16e13058e0e0e534f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n",
            "Successfully built deepspeed transformers_stream_generator\n",
            "Installing collected packages: tokenizers, nvidia-ml-py, ninja, hjson, xxhash, rouge, pydantic, pyarrow, humanfriendly, gekko, dill, tiktoken, multiprocess, coloredlogs, transformers, deepspeed, transformers_stream_generator, peft, datasets, optimum, auto-gptq\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.2\n",
            "    Uninstalling pydantic-2.9.2:\n",
            "      Successfully uninstalled pydantic-2.9.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.15 requires pydantic>=2.7.0, but you have pydantic 1.10.13 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed auto-gptq-0.7.1 coloredlogs-15.0.1 datasets-3.0.0 deepspeed-0.14.5 dill-0.3.8 gekko-1.2.1 hjson-3.1.0 humanfriendly-10.0 multiprocess-0.70.16 ninja-1.11.1.1 nvidia-ml-py-12.560.30 optimum-1.22.0 peft-0.12.0 pyarrow-17.0.0 pydantic-1.10.13 rouge-1.0.1 tiktoken-0.7.0 tokenizers-0.13.3 transformers-4.32.0 transformers_stream_generator-0.0.5 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K8F9uaHBTZ0",
        "outputId": "00af0b26-f6f7-4ea5-f34b-1f6531600065"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.0.tar.gz (464 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/464.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4266269 sha256=fdc6987416796be890b5067dfa3db0a9bcab054031a601a1846d54d9472d126e\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "df = pd.read_json('https://raw.githubusercontent.com/wangyuxinwhy/uniem/main/examples/example_data/riddle.jsonl', lines=True)\n",
        "df = df.rename(columns={'instruction': 'user', 'output': 'assistant'})\n",
        "\n",
        "chat_input = df[\"user\"].tolist()\n",
        "chat_output = df[\"assistant\"].tolist()\n",
        "\n",
        "all_chat = []\n",
        "for i in range(len(chat_input)):\n",
        "    user = str(chat_input[i])\n",
        "    assistant = str(chat_output[i])\n",
        "    conversations = [{\"from\": \"user\", \"value\": user}, {\"from\": \"assistant\", \"value\": assistant}]\n",
        "    chat_id = \"identity_\" + str(i)\n",
        "    chat = {\"id\": chat_id, \"conversations\": conversations}\n",
        "    all_chat.append(chat)\n",
        "\n",
        "file_path = 'chat.json'\n",
        "\n",
        "with open(file_path, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(all_chat, json_file, ensure_ascii=False)\n",
        "all_chat[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvFw3_kxBXHS",
        "outputId": "6f926e57-056d-4b75-f1ad-09ef42730794"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'identity_0',\n",
              " 'conversations': [{'from': 'user',\n",
              "   'value': '猜谜语：一身卷卷细毛，吃的青青野草，过了数九寒冬，无私献出白毛。 （打一动物）'},\n",
              "  {'from': 'assistant', 'value': '谜底：白羊'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_chat[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKg2LxrNBitJ",
        "outputId": "33ff8a18-8ae2-437e-8568-7572ba6e660c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'identity_0',\n",
              " 'conversations': [{'from': 'user',\n",
              "   'value': '猜谜语：一身卷卷细毛，吃的青青野草，过了数九寒冬，无私献出白毛。 （打一动物）'},\n",
              "  {'from': 'assistant', 'value': '谜底：白羊'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://modelscope.cn/qwen/Qwen-1_8B-Chat-Int4.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWLgk4SzFk33",
        "outputId": "f2fde2e8-a22d-4430-95bc-2edee0659061"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Qwen-1_8B-Chat-Int4'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 119 (delta 39), reused 112 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (119/119), 16.18 MiB | 13.38 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
        "\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "!python finetune.py \\\n",
        "  --model_name_or_path Qwen-1_8B-Chat-Int4 \\\n",
        "  --data_path chat.json \\\n",
        "  --fp16 True \\\n",
        "  --output_dir output_qwen \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --per_device_train_batch_size 2 \\\n",
        "  --per_device_eval_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 8 \\\n",
        "  --evaluation_strategy \"no\" \\\n",
        "  --save_strategy \"steps\" \\\n",
        "  --save_steps 1000 \\\n",
        "  --save_total_limit 10 \\\n",
        "  --learning_rate 3e-4 \\\n",
        "  --weight_decay 0.1 \\\n",
        "  --adam_beta2 0.95 \\\n",
        "  --warmup_ratio 0.01 \\\n",
        "  --lr_scheduler_type \"cosine\" \\\n",
        "  --logging_steps 1 \\\n",
        "  --report_to \"none\" \\\n",
        "  --model_max_length 512 \\\n",
        "  --lazy_preprocess True \\\n",
        "  --gradient_checkpointing \\\n",
        "  --use_lora \\\n",
        "  --q_lora \\\n",
        "  --deepspeed finetune/ds_config_zero2.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWufCyjIFnvU",
        "outputId": "bc0bf66d-4031-4d38-ad37-2724f6f7b53a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-09-23 18:23:05,846] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, weight, bias=None):\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "2024-09-23 18:23:10.321682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-23 18:23:10.342772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-23 18:23:10.349205: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-23 18:23:11.453314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "Try importing flash-attention for faster inference...\n",
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Some weights of the model checkpoint at Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.0.attn.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias']\n",
            "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "trainable params: 53,673,984 || all params: 676,104,192 || trainable%: 7.9387\n",
            "Loading data...\n",
            "Formatting inputs...Skip in lazy mode\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
            "  warnings.warn(\n",
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py310_cu121/fused_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
            "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 32.870529890060425 seconds\n",
            "  0% 0/62 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "{'loss': 1.2358, 'learning_rate': 0.0, 'epoch': 0.02}\n",
            "{'loss': 0.6379, 'learning_rate': 0.0003, 'epoch': 0.03}\n",
            "{'loss': 0.6636, 'learning_rate': 0.0003, 'epoch': 0.05}\n",
            "{'loss': 0.6614, 'learning_rate': 0.0003, 'epoch': 0.06}\n",
            "{'loss': 0.8824, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
            "{'loss': 0.6183, 'learning_rate': 0.0003, 'epoch': 0.1}\n",
            "{'loss': 0.6353, 'learning_rate': 0.0003, 'epoch': 0.11}\n",
            "{'loss': 0.7053, 'learning_rate': 0.0003, 'epoch': 0.13}\n",
            "{'loss': 0.4812, 'learning_rate': 0.0003, 'epoch': 0.14}\n",
            "{'loss': 0.6197, 'learning_rate': 0.0003, 'epoch': 0.16}\n",
            "{'loss': 0.6127, 'learning_rate': 0.0003, 'epoch': 0.18}\n",
            "{'loss': 0.5756, 'learning_rate': 0.0003, 'epoch': 0.19}\n",
            "{'loss': 0.5482, 'learning_rate': 0.0003, 'epoch': 0.21}\n",
            "{'loss': 0.8309, 'learning_rate': 0.0003, 'epoch': 0.22}\n",
            "{'loss': 0.6266, 'learning_rate': 0.0003, 'epoch': 0.24}\n",
            "{'loss': 0.4311, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
            "{'loss': 0.6912, 'learning_rate': 0.0003, 'epoch': 0.27}\n",
            "{'loss': 0.614, 'learning_rate': 0.0003, 'epoch': 0.29}\n",
            "{'loss': 0.7325, 'learning_rate': 0.0003, 'epoch': 0.3}\n",
            "{'loss': 0.5782, 'learning_rate': 0.0003, 'epoch': 0.32}\n",
            "{'loss': 0.6171, 'learning_rate': 0.0003, 'epoch': 0.34}\n",
            "{'loss': 0.6855, 'learning_rate': 0.0003, 'epoch': 0.35}\n",
            "{'loss': 0.6517, 'learning_rate': 0.0003, 'epoch': 0.37}\n",
            "{'loss': 0.6529, 'learning_rate': 0.0003, 'epoch': 0.38}\n",
            "{'loss': 0.5002, 'learning_rate': 0.0003, 'epoch': 0.4}\n",
            "{'loss': 0.511, 'learning_rate': 0.0003, 'epoch': 0.42}\n",
            "{'loss': 0.4693, 'learning_rate': 0.0003, 'epoch': 0.43}\n",
            "{'loss': 0.6296, 'learning_rate': 0.0003, 'epoch': 0.45}\n",
            "{'loss': 0.4626, 'learning_rate': 0.0003, 'epoch': 0.46}\n",
            "{'loss': 0.8615, 'learning_rate': 0.0003, 'epoch': 0.48}\n",
            "{'loss': 0.4848, 'learning_rate': 0.0003, 'epoch': 0.5}\n",
            "{'loss': 0.8733, 'learning_rate': 0.0003, 'epoch': 0.51}\n",
            "{'loss': 0.4781, 'learning_rate': 0.0003, 'epoch': 0.53}\n",
            "{'loss': 0.5711, 'learning_rate': 0.0003, 'epoch': 0.54}\n",
            "{'loss': 0.4422, 'learning_rate': 0.0003, 'epoch': 0.56}\n",
            "{'loss': 0.556, 'learning_rate': 0.0003, 'epoch': 0.58}\n",
            "{'loss': 0.6429, 'learning_rate': 0.0003, 'epoch': 0.59}\n",
            "{'loss': 0.6834, 'learning_rate': 0.0003, 'epoch': 0.61}\n",
            "{'loss': 0.6355, 'learning_rate': 0.0003, 'epoch': 0.62}\n",
            "{'loss': 0.5371, 'learning_rate': 0.0003, 'epoch': 0.64}\n",
            "{'loss': 0.6353, 'learning_rate': 0.0003, 'epoch': 0.66}\n",
            "{'loss': 0.6528, 'learning_rate': 0.0003, 'epoch': 0.67}\n",
            "{'loss': 0.6682, 'learning_rate': 0.0003, 'epoch': 0.69}\n",
            "{'loss': 0.5684, 'learning_rate': 0.0003, 'epoch': 0.7}\n",
            "{'loss': 0.4111, 'learning_rate': 0.0003, 'epoch': 0.72}\n",
            "{'loss': 0.6893, 'learning_rate': 0.0003, 'epoch': 0.74}\n",
            "{'loss': 0.6184, 'learning_rate': 0.0003, 'epoch': 0.75}\n",
            "{'loss': 0.6804, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
            "{'loss': 0.5619, 'learning_rate': 0.0003, 'epoch': 0.78}\n",
            "{'loss': 0.7026, 'learning_rate': 0.0003, 'epoch': 0.8}\n",
            "{'loss': 0.7503, 'learning_rate': 0.0003, 'epoch': 0.82}\n",
            "{'loss': 0.6581, 'learning_rate': 0.0003, 'epoch': 0.83}\n",
            "{'loss': 0.5102, 'learning_rate': 0.0003, 'epoch': 0.85}\n",
            "{'loss': 0.6302, 'learning_rate': 0.0003, 'epoch': 0.86}\n",
            "{'loss': 0.7032, 'learning_rate': 0.0003, 'epoch': 0.88}\n",
            "{'loss': 0.5683, 'learning_rate': 0.0003, 'epoch': 0.9}\n",
            "{'loss': 0.4656, 'learning_rate': 0.0003, 'epoch': 0.91}\n",
            "{'loss': 0.6785, 'learning_rate': 0.0003, 'epoch': 0.93}\n",
            "{'loss': 0.7493, 'learning_rate': 0.0003, 'epoch': 0.94}\n",
            "{'loss': 0.523, 'learning_rate': 0.0003, 'epoch': 0.96}\n",
            "{'loss': 0.8436, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
            "{'loss': 0.5542, 'learning_rate': 0.0003, 'epoch': 0.99}\n",
            "{'train_runtime': 254.1475, 'train_samples_per_second': 3.935, 'train_steps_per_second': 0.244, 'train_loss': 0.6314592669087071, 'epoch': 0.99}\n",
            "100% 62/62 [04:14<00:00,  4.10s/it]\n",
            "[rank0]:[W923 18:28:06.633562749 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\"output_qwen\", device_map=\"auto\", trust_remote_code=True).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"output_qwen\", trust_remote_code=True)\n",
        "response, history = model.chat(tokenizer, \"猜谜语：一身卷卷细毛，吃的青青野草，过了数九寒冬，无私献出白毛。 （打一动物）\", history=None)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PUEzWAyFuzZ",
        "outputId": "270604de-6b43-4da4-ff19-2d068bbd52ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.Qwen-1_8B-Chat-Int4.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:transformers_modules.Qwen-1_8B-Chat-Int4.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:transformers_modules.Qwen-1_8B-Chat-Int4.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:transformers_modules.Qwen-1_8B-Chat-Int4.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Some weights of the model checkpoint at Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.6.mlp.w1.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias']\n",
            "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 151851. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "谜底：老牛\n"
          ]
        }
      ]
    }
  ]
}